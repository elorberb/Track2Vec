{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw files paths\n",
    "saving_data_path = \"/home/etaylor/code_projects/track2vec/data\"\n",
    "evalrs_events_raw = \"/home/etaylor/.cache/evalrs/LFM-1b_LEs.txt\"\n",
    "evalrs_tracks_raw = \"/home/etaylor/.cache/evalrs/LFM-1b_tracks.txt\"\n",
    "evalrs_users_raw = \"/home/etaylor/.cache/evalrs/LFM-1b_users.txt\"\n",
    "\n",
    "# data headers\n",
    "events_headers = ['user_id', 'artist_id', 'album_id', 'track_id', 'timestamp']\n",
    "tracks_headers = ['track_id', 'track_name', 'artist_id']\n",
    "users_headers = ['user_id', 'country', 'age', 'gender', 'playcount', 'timestamp']\n",
    "\n",
    "# processed files paths\n",
    "users_filepath = f\"{saving_data_path}/evalrs_users.csv\"\n",
    "tracks_filepath = f\"{saving_data_path}/evalrs_tracks.csv\"\n",
    "events_filepath = f\"{saving_data_path}/evalrs_events.csv\"\n",
    "output_filepath = f'{saving_data_path}/final_training_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_relevant_events_data(events_filepath, users_filepath, output_filepath, chunksize=500000, verbose=False, max_size_gb=1):\n",
    "    users_headers = ['user_id', 'country', 'age', 'gender', 'playcount', 'registered']\n",
    "    events_headers = ['user_id', 'artist_id', 'album_id', 'track_id', 'timestamp']\n",
    "    \n",
    "    # Load the users data with the correct headers\n",
    "    users_df = pd.read_csv(users_filepath, names=users_headers, skiprows=1, sep='\\t')\n",
    "    user_ids = set(users_df['user_id'])\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Loaded users data:\")\n",
    "        print(users_df.head())\n",
    "\n",
    "    # Initialize an empty DataFrame to accumulate relevant events\n",
    "    relevant_events_df = pd.DataFrame()\n",
    "\n",
    "    chunk_counter = 0\n",
    "    total_size_bytes = 0\n",
    "    max_size_bytes = max_size_gb * 1024 ** 3  # Convert GB to bytes\n",
    "\n",
    "    for chunk in pd.read_csv(events_filepath, chunksize=chunksize, sep='\\t', names=events_headers, skiprows=1):\n",
    "        filtered_chunk = chunk[chunk['user_id'].isin(user_ids)]\n",
    "        total_size_bytes += filtered_chunk.memory_usage(deep=True).sum()\n",
    "        \n",
    "        # If the total size exceeds the limit, save the current data and reset\n",
    "        if total_size_bytes >= max_size_bytes:\n",
    "            temp_output_path = f\"{output_filepath}_part_{chunk_counter}.csv\"\n",
    "            relevant_events_df.to_csv(temp_output_path, index=False, mode='a', header=not os.path.exists(temp_output_path))\n",
    "            if verbose:\n",
    "                print(f\"Saved partial relevant events data to {temp_output_path} and reset the DataFrame\")\n",
    "            \n",
    "            # Reset for the next iteration\n",
    "            relevant_events_df = pd.DataFrame()\n",
    "            total_size_bytes = 0\n",
    "\n",
    "        relevant_events_df = pd.concat([relevant_events_df, filtered_chunk])\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Processed chunk {chunk_counter}.\")\n",
    "        \n",
    "        chunk_counter += 1\n",
    "\n",
    "    # Save any remaining data to disk\n",
    "    if not relevant_events_df.empty:\n",
    "        temp_output_path = f\"{output_filepath}_part_{chunk_counter}.csv\"\n",
    "        relevant_events_df.to_csv(temp_output_path, index=False, mode='a', header=not os.path.exists(temp_output_path))\n",
    "        if verbose:\n",
    "            print(f\"Saved final part of relevant events data to {temp_output_path}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Finished processing all chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded users data:\n",
      "   user_id country  age gender  playcount  registered\n",
      "0      384      UK   35      m      42139  1035849600\n",
      "1     1206     NaN   -1      n      33103  1035849600\n",
      "2     2622     NaN   -1    NaN       2030  1037404800\n",
      "3     2732     NaN   -1      n        147  1037577600\n",
      "4     3653      UK   31      m      18504  1041033600\n",
      "Processed chunk 0.\n",
      "Processed chunk 1.\n",
      "Processed chunk 2.\n",
      "Processed chunk 3.\n",
      "Processed chunk 4.\n",
      "Processed chunk 5.\n",
      "Processed chunk 6.\n",
      "Processed chunk 7.\n",
      "Processed chunk 8.\n",
      "Processed chunk 9.\n",
      "Processed chunk 10.\n",
      "Processed chunk 11.\n",
      "Processed chunk 12.\n",
      "Processed chunk 13.\n",
      "Processed chunk 14.\n",
      "Processed chunk 15.\n",
      "Processed chunk 16.\n",
      "Processed chunk 17.\n",
      "Processed chunk 18.\n",
      "Processed chunk 19.\n",
      "Processed chunk 20.\n",
      "Processed chunk 21.\n",
      "Processed chunk 22.\n",
      "Processed chunk 23.\n",
      "Processed chunk 24.\n",
      "Processed chunk 25.\n",
      "Processed chunk 26.\n",
      "Processed chunk 27.\n",
      "Processed chunk 28.\n",
      "Processed chunk 29.\n",
      "Processed chunk 30.\n",
      "Processed chunk 31.\n",
      "Processed chunk 32.\n",
      "Processed chunk 33.\n",
      "Processed chunk 34.\n",
      "Processed chunk 35.\n",
      "Processed chunk 36.\n",
      "Processed chunk 37.\n",
      "Processed chunk 38.\n",
      "Processed chunk 39.\n",
      "Processed chunk 40.\n",
      "Processed chunk 41.\n",
      "Processed chunk 42.\n",
      "Processed chunk 43.\n",
      "Processed chunk 44.\n",
      "Processed chunk 45.\n",
      "Processed chunk 46.\n",
      "Processed chunk 47.\n",
      "Processed chunk 48.\n",
      "Processed chunk 49.\n",
      "Processed chunk 50.\n",
      "Processed chunk 51.\n",
      "Processed chunk 52.\n",
      "Saved partial relevant events data to /home/etaylor/code_projects/track2vec/data/final_training_dataset.csv_part_53.csv and reset the DataFrame\n",
      "Processed chunk 53.\n",
      "Processed chunk 54.\n",
      "Processed chunk 55.\n",
      "Processed chunk 56.\n",
      "Processed chunk 57.\n",
      "Processed chunk 58.\n",
      "Processed chunk 59.\n",
      "Processed chunk 60.\n",
      "Processed chunk 61.\n",
      "Processed chunk 62.\n",
      "Processed chunk 63.\n",
      "Processed chunk 64.\n",
      "Processed chunk 65.\n",
      "Processed chunk 66.\n",
      "Processed chunk 67.\n",
      "Processed chunk 68.\n",
      "Processed chunk 69.\n",
      "Processed chunk 70.\n",
      "Processed chunk 71.\n",
      "Processed chunk 72.\n",
      "Processed chunk 73.\n",
      "Processed chunk 74.\n",
      "Processed chunk 75.\n",
      "Processed chunk 76.\n",
      "Processed chunk 77.\n",
      "Processed chunk 78.\n",
      "Processed chunk 79.\n",
      "Processed chunk 80.\n",
      "Processed chunk 81.\n",
      "Processed chunk 82.\n",
      "Processed chunk 83.\n",
      "Processed chunk 84.\n",
      "Processed chunk 85.\n",
      "Processed chunk 86.\n",
      "Processed chunk 87.\n",
      "Processed chunk 88.\n",
      "Processed chunk 89.\n",
      "Processed chunk 90.\n",
      "Processed chunk 91.\n",
      "Processed chunk 92.\n",
      "Processed chunk 93.\n",
      "Processed chunk 94.\n",
      "Processed chunk 95.\n",
      "Processed chunk 96.\n",
      "Processed chunk 97.\n",
      "Processed chunk 98.\n",
      "Processed chunk 99.\n",
      "Processed chunk 100.\n",
      "Processed chunk 101.\n",
      "Processed chunk 102.\n",
      "Processed chunk 103.\n",
      "Processed chunk 104.\n",
      "Processed chunk 105.\n",
      "Processed chunk 106.\n",
      "Saved partial relevant events data to /home/etaylor/code_projects/track2vec/data/final_training_dataset.csv_part_107.csv and reset the DataFrame\n",
      "Processed chunk 107.\n",
      "Processed chunk 108.\n",
      "Processed chunk 109.\n",
      "Processed chunk 110.\n",
      "Processed chunk 111.\n",
      "Processed chunk 112.\n",
      "Processed chunk 113.\n",
      "Processed chunk 114.\n",
      "Processed chunk 115.\n",
      "Processed chunk 116.\n",
      "Processed chunk 117.\n",
      "Processed chunk 118.\n",
      "Processed chunk 119.\n",
      "Processed chunk 120.\n",
      "Processed chunk 121.\n",
      "Processed chunk 122.\n",
      "Processed chunk 123.\n",
      "Processed chunk 124.\n",
      "Processed chunk 125.\n",
      "Processed chunk 126.\n",
      "Processed chunk 127.\n",
      "Processed chunk 128.\n",
      "Processed chunk 129.\n",
      "Processed chunk 130.\n",
      "Processed chunk 131.\n",
      "Processed chunk 132.\n",
      "Processed chunk 133.\n",
      "Processed chunk 134.\n",
      "Processed chunk 135.\n",
      "Processed chunk 136.\n",
      "Processed chunk 137.\n",
      "Processed chunk 138.\n",
      "Processed chunk 139.\n",
      "Processed chunk 140.\n",
      "Processed chunk 141.\n",
      "Processed chunk 142.\n",
      "Processed chunk 143.\n",
      "Processed chunk 144.\n",
      "Processed chunk 145.\n",
      "Processed chunk 146.\n",
      "Processed chunk 147.\n",
      "Processed chunk 148.\n",
      "Processed chunk 149.\n",
      "Processed chunk 150.\n",
      "Processed chunk 151.\n",
      "Processed chunk 152.\n",
      "Processed chunk 153.\n",
      "Processed chunk 154.\n",
      "Processed chunk 155.\n",
      "Processed chunk 156.\n",
      "Processed chunk 157.\n",
      "Processed chunk 158.\n",
      "Processed chunk 159.\n",
      "Processed chunk 160.\n",
      "Saved partial relevant events data to /home/etaylor/code_projects/track2vec/data/final_training_dataset.csv_part_161.csv and reset the DataFrame\n",
      "Processed chunk 161.\n",
      "Processed chunk 162.\n",
      "Processed chunk 163.\n",
      "Processed chunk 164.\n",
      "Processed chunk 165.\n",
      "Processed chunk 166.\n",
      "Processed chunk 167.\n",
      "Processed chunk 168.\n",
      "Processed chunk 169.\n",
      "Processed chunk 170.\n",
      "Processed chunk 171.\n",
      "Processed chunk 172.\n",
      "Processed chunk 173.\n",
      "Processed chunk 174.\n",
      "Processed chunk 175.\n",
      "Processed chunk 176.\n",
      "Processed chunk 177.\n",
      "Processed chunk 178.\n",
      "Processed chunk 179.\n",
      "Processed chunk 180.\n",
      "Processed chunk 181.\n",
      "Processed chunk 182.\n",
      "Processed chunk 183.\n",
      "Processed chunk 184.\n",
      "Processed chunk 185.\n",
      "Processed chunk 186.\n",
      "Processed chunk 187.\n",
      "Processed chunk 188.\n",
      "Processed chunk 189.\n",
      "Processed chunk 190.\n",
      "Processed chunk 191.\n",
      "Processed chunk 192.\n",
      "Processed chunk 193.\n",
      "Processed chunk 194.\n",
      "Processed chunk 195.\n",
      "Processed chunk 196.\n",
      "Processed chunk 197.\n",
      "Processed chunk 198.\n",
      "Processed chunk 199.\n",
      "Processed chunk 200.\n",
      "Processed chunk 201.\n",
      "Processed chunk 202.\n",
      "Processed chunk 203.\n",
      "Processed chunk 204.\n",
      "Processed chunk 205.\n",
      "Processed chunk 206.\n",
      "Processed chunk 207.\n",
      "Processed chunk 208.\n",
      "Processed chunk 209.\n",
      "Processed chunk 210.\n",
      "Processed chunk 211.\n",
      "Processed chunk 212.\n",
      "Processed chunk 213.\n",
      "Processed chunk 214.\n",
      "Saved partial relevant events data to /home/etaylor/code_projects/track2vec/data/final_training_dataset.csv_part_215.csv and reset the DataFrame\n",
      "Processed chunk 215.\n",
      "Processed chunk 216.\n",
      "Processed chunk 217.\n",
      "Processed chunk 218.\n",
      "Processed chunk 219.\n",
      "Processed chunk 220.\n",
      "Processed chunk 221.\n",
      "Processed chunk 222.\n",
      "Processed chunk 223.\n",
      "Processed chunk 224.\n",
      "Processed chunk 225.\n",
      "Processed chunk 226.\n",
      "Processed chunk 227.\n",
      "Processed chunk 228.\n",
      "Processed chunk 229.\n",
      "Processed chunk 230.\n",
      "Processed chunk 231.\n",
      "Processed chunk 232.\n",
      "Processed chunk 233.\n",
      "Processed chunk 234.\n",
      "Processed chunk 235.\n",
      "Processed chunk 236.\n",
      "Processed chunk 237.\n",
      "Processed chunk 238.\n",
      "Processed chunk 239.\n",
      "Processed chunk 240.\n",
      "Processed chunk 241.\n",
      "Processed chunk 242.\n",
      "Processed chunk 243.\n",
      "Processed chunk 244.\n",
      "Processed chunk 245.\n",
      "Processed chunk 246.\n",
      "Processed chunk 247.\n",
      "Processed chunk 248.\n",
      "Processed chunk 249.\n",
      "Processed chunk 250.\n",
      "Processed chunk 251.\n",
      "Processed chunk 252.\n",
      "Processed chunk 253.\n",
      "Processed chunk 254.\n",
      "Processed chunk 255.\n",
      "Processed chunk 256.\n",
      "Processed chunk 257.\n",
      "Processed chunk 258.\n",
      "Processed chunk 259.\n",
      "Processed chunk 260.\n",
      "Processed chunk 261.\n",
      "Processed chunk 262.\n",
      "Processed chunk 263.\n",
      "Processed chunk 264.\n",
      "Processed chunk 265.\n",
      "Processed chunk 266.\n",
      "Processed chunk 267.\n",
      "Processed chunk 268.\n",
      "Saved partial relevant events data to /home/etaylor/code_projects/track2vec/data/final_training_dataset.csv_part_269.csv and reset the DataFrame\n",
      "Processed chunk 269.\n",
      "Processed chunk 270.\n",
      "Processed chunk 271.\n",
      "Processed chunk 272.\n",
      "Processed chunk 273.\n",
      "Processed chunk 274.\n",
      "Processed chunk 275.\n",
      "Processed chunk 276.\n",
      "Processed chunk 277.\n",
      "Processed chunk 278.\n",
      "Processed chunk 279.\n",
      "Processed chunk 280.\n",
      "Processed chunk 281.\n",
      "Processed chunk 282.\n",
      "Processed chunk 283.\n",
      "Processed chunk 284.\n",
      "Processed chunk 285.\n",
      "Processed chunk 286.\n",
      "Processed chunk 287.\n",
      "Processed chunk 288.\n",
      "Processed chunk 289.\n",
      "Processed chunk 290.\n",
      "Processed chunk 291.\n",
      "Processed chunk 292.\n",
      "Processed chunk 293.\n",
      "Processed chunk 294.\n",
      "Processed chunk 295.\n",
      "Processed chunk 296.\n",
      "Processed chunk 297.\n",
      "Processed chunk 298.\n",
      "Processed chunk 299.\n",
      "Processed chunk 300.\n",
      "Processed chunk 301.\n",
      "Processed chunk 302.\n",
      "Processed chunk 303.\n",
      "Processed chunk 304.\n",
      "Processed chunk 305.\n",
      "Processed chunk 306.\n",
      "Processed chunk 307.\n",
      "Processed chunk 308.\n",
      "Processed chunk 309.\n",
      "Processed chunk 310.\n",
      "Processed chunk 311.\n",
      "Processed chunk 312.\n",
      "Processed chunk 313.\n",
      "Processed chunk 314.\n",
      "Processed chunk 315.\n",
      "Processed chunk 316.\n",
      "Processed chunk 317.\n",
      "Processed chunk 318.\n",
      "Processed chunk 319.\n",
      "Processed chunk 320.\n",
      "Processed chunk 321.\n",
      "Processed chunk 322.\n",
      "Saved partial relevant events data to /home/etaylor/code_projects/track2vec/data/final_training_dataset.csv_part_323.csv and reset the DataFrame\n",
      "Processed chunk 323.\n",
      "Processed chunk 324.\n",
      "Processed chunk 325.\n",
      "Processed chunk 326.\n",
      "Processed chunk 327.\n",
      "Processed chunk 328.\n",
      "Processed chunk 329.\n",
      "Processed chunk 330.\n",
      "Processed chunk 331.\n",
      "Processed chunk 332.\n",
      "Processed chunk 333.\n",
      "Processed chunk 334.\n",
      "Processed chunk 335.\n",
      "Processed chunk 336.\n",
      "Processed chunk 337.\n",
      "Processed chunk 338.\n",
      "Processed chunk 339.\n",
      "Processed chunk 340.\n",
      "Processed chunk 341.\n",
      "Processed chunk 342.\n",
      "Processed chunk 343.\n",
      "Processed chunk 344.\n",
      "Processed chunk 345.\n",
      "Processed chunk 346.\n",
      "Processed chunk 347.\n",
      "Processed chunk 348.\n",
      "Processed chunk 349.\n",
      "Processed chunk 350.\n",
      "Processed chunk 351.\n",
      "Processed chunk 352.\n",
      "Processed chunk 353.\n",
      "Processed chunk 354.\n",
      "Processed chunk 355.\n",
      "Processed chunk 356.\n",
      "Processed chunk 357.\n",
      "Processed chunk 358.\n",
      "Processed chunk 359.\n",
      "Processed chunk 360.\n",
      "Processed chunk 361.\n",
      "Processed chunk 362.\n",
      "Processed chunk 363.\n",
      "Processed chunk 364.\n",
      "Processed chunk 365.\n",
      "Processed chunk 366.\n",
      "Processed chunk 367.\n",
      "Processed chunk 368.\n",
      "Processed chunk 369.\n",
      "Processed chunk 370.\n",
      "Processed chunk 371.\n",
      "Processed chunk 372.\n",
      "Processed chunk 373.\n",
      "Processed chunk 374.\n",
      "Processed chunk 375.\n",
      "Processed chunk 376.\n",
      "Saved partial relevant events data to /home/etaylor/code_projects/track2vec/data/final_training_dataset.csv_part_377.csv and reset the DataFrame\n",
      "Processed chunk 377.\n",
      "Processed chunk 378.\n",
      "Processed chunk 379.\n",
      "Processed chunk 380.\n",
      "Processed chunk 381.\n",
      "Processed chunk 382.\n",
      "Processed chunk 383.\n",
      "Processed chunk 384.\n",
      "Processed chunk 385.\n",
      "Processed chunk 386.\n",
      "Processed chunk 387.\n",
      "Processed chunk 388.\n",
      "Processed chunk 389.\n",
      "Processed chunk 390.\n",
      "Processed chunk 391.\n",
      "Processed chunk 392.\n",
      "Processed chunk 393.\n",
      "Processed chunk 394.\n",
      "Processed chunk 395.\n",
      "Processed chunk 396.\n",
      "Processed chunk 397.\n",
      "Processed chunk 398.\n",
      "Processed chunk 399.\n",
      "Processed chunk 400.\n",
      "Processed chunk 401.\n",
      "Processed chunk 402.\n",
      "Processed chunk 403.\n",
      "Processed chunk 404.\n",
      "Processed chunk 405.\n",
      "Processed chunk 406.\n",
      "Processed chunk 407.\n",
      "Processed chunk 408.\n",
      "Processed chunk 409.\n",
      "Processed chunk 410.\n",
      "Processed chunk 411.\n",
      "Processed chunk 412.\n",
      "Processed chunk 413.\n",
      "Processed chunk 414.\n",
      "Processed chunk 415.\n",
      "Processed chunk 416.\n",
      "Processed chunk 417.\n",
      "Processed chunk 418.\n",
      "Processed chunk 419.\n",
      "Processed chunk 420.\n",
      "Processed chunk 421.\n",
      "Processed chunk 422.\n",
      "Processed chunk 423.\n",
      "Processed chunk 424.\n",
      "Processed chunk 425.\n",
      "Processed chunk 426.\n",
      "Processed chunk 427.\n",
      "Processed chunk 428.\n",
      "Processed chunk 429.\n",
      "Processed chunk 430.\n",
      "Saved partial relevant events data to /home/etaylor/code_projects/track2vec/data/final_training_dataset.csv_part_431.csv and reset the DataFrame\n",
      "Processed chunk 431.\n",
      "Processed chunk 432.\n",
      "Processed chunk 433.\n",
      "Processed chunk 434.\n",
      "Processed chunk 435.\n",
      "Processed chunk 436.\n",
      "Processed chunk 437.\n",
      "Processed chunk 438.\n",
      "Processed chunk 439.\n",
      "Processed chunk 440.\n",
      "Processed chunk 441.\n",
      "Processed chunk 442.\n",
      "Processed chunk 443.\n",
      "Processed chunk 444.\n",
      "Processed chunk 445.\n",
      "Processed chunk 446.\n",
      "Processed chunk 447.\n",
      "Processed chunk 448.\n",
      "Processed chunk 449.\n",
      "Processed chunk 450.\n",
      "Processed chunk 451.\n",
      "Processed chunk 452.\n",
      "Processed chunk 453.\n",
      "Processed chunk 454.\n",
      "Processed chunk 455.\n",
      "Processed chunk 456.\n",
      "Processed chunk 457.\n",
      "Processed chunk 458.\n",
      "Processed chunk 459.\n",
      "Processed chunk 460.\n",
      "Processed chunk 461.\n",
      "Processed chunk 462.\n",
      "Processed chunk 463.\n",
      "Processed chunk 464.\n",
      "Processed chunk 465.\n",
      "Processed chunk 466.\n",
      "Processed chunk 467.\n",
      "Processed chunk 468.\n",
      "Processed chunk 469.\n",
      "Processed chunk 470.\n",
      "Processed chunk 471.\n",
      "Processed chunk 472.\n",
      "Processed chunk 473.\n",
      "Processed chunk 474.\n",
      "Processed chunk 475.\n",
      "Processed chunk 476.\n",
      "Processed chunk 477.\n",
      "Processed chunk 478.\n",
      "Processed chunk 479.\n",
      "Processed chunk 480.\n",
      "Processed chunk 481.\n",
      "Processed chunk 482.\n",
      "Processed chunk 483.\n",
      "Processed chunk 484.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Call the function to create the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcreate_training_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevalrs_events_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalrs_users_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mcreate_training_dataset\u001b[0;34m(events_filepath, users_filepath, output_filepath, chunksize, verbose, max_size_gb)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_size_bytes \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_size_bytes:\n\u001b[1;32m     28\u001b[0m     temp_output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_filepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_part_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_counter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mrelevant_events_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_output_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_output_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved partial relevant events data to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_output_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and reset the DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/track2vec/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/track2vec/lib/python3.10/site-packages/pandas/core/generic.py:3961\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3950\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3952\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3953\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3954\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3958\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3959\u001b[0m )\n\u001b[0;32m-> 3961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3964\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/track2vec/lib/python3.10/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/.conda/envs/track2vec/lib/python3.10/site-packages/pandas/io/formats/csvs.py:270\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/track2vec/lib/python3.10/site-packages/pandas/io/formats/csvs.py:275\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_to_save_header:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_header()\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/track2vec/lib/python3.10/site-packages/pandas/io/formats/csvs.py:313\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_i:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/track2vec/lib/python3.10/site-packages/pandas/io/formats/csvs.py:324\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(res\u001b[38;5;241m.\u001b[39m_iter_column_arrays())\n\u001b[1;32m    323\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_index[slicer]\u001b[38;5;241m.\u001b[39m_get_values_for_csv(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n\u001b[0;32m--> 324\u001b[0m \u001b[43mlibwriters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_csv_rows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mwriters.pyx:73\u001b[0m, in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "extract_relevant_events_data(evalrs_events_raw, evalrs_users_raw, output_filepath, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_training_dataset(events_filepath, users_filepath, output_filepath, verbose=False):\n",
    "    # Load the events data\n",
    "    events_df = pd.read_csv(events_filepath)\n",
    "    if verbose:\n",
    "        print(\"Events DataFrame head:\\n\", events_df.head())\n",
    "\n",
    "    # Aggregate track IDs into a list per user\n",
    "    user_tracks_list_df = events_df.groupby('user_id')['track_id'].apply(list).reset_index(name='track_id')\n",
    "    if verbose:\n",
    "        print(\"User-Tracks List DataFrame head:\\n\", user_tracks_list_df.head())\n",
    "\n",
    "    # Count the distinct number of tracks each user has listened to\n",
    "    user_track_distinct_count_df = events_df.groupby('user_id')['track_id'].nunique().reset_index(name='distinct_track_count')\n",
    "    if verbose:\n",
    "        print(\"User-Distinct Track Count DataFrame head:\\n\", user_track_distinct_count_df.head())\n",
    "\n",
    "    # Merge the track IDs list and distinct track count with the users data\n",
    "    users_df = pd.read_csv(users_filepath)\n",
    "    if verbose:\n",
    "        print(\"Users DataFrame head:\\n\", users_df.head())\n",
    "\n",
    "    # Merge users data with track IDs list\n",
    "    users_with_tracks_df = users_df.merge(user_tracks_list_df, on='user_id', how='left')\n",
    "    \n",
    "    # Merge the above result with the distinct track count\n",
    "    final_df = users_with_tracks_df.merge(user_track_distinct_count_df, on='user_id', how='left')\n",
    "    if verbose:\n",
    "        print(\"Users with Track IDs and Distinct Counts DataFrame head:\\n\", final_df.head())\n",
    "\n",
    "    # Drop duplicates to avoid repetition for each user\n",
    "    final_df = final_df.drop_duplicates(subset='user_id')\n",
    "\n",
    "    # Select the columns of interest\n",
    "    final_df = final_df[['user_id', 'playcount', 'gender', 'distinct_track_count', 'track_id']]\n",
    "    if verbose:\n",
    "        print(\"Final DataFrame head:\\n\", final_df.head())\n",
    "        \n",
    "    # Drop records with missing values if necessary\n",
    "    final_df = final_df.dropna(axis=0)\n",
    "\n",
    "    # Save the final dataframe to a CSV file\n",
    "    final_df.to_csv(output_filepath, index=False)\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events DataFrame head:\n",
      "     user_id  artist_id  album_id  track_id   timestamp\n",
      "0  20926447       3983     10756    789533  1353509220\n",
      "1  20926447       3983     10756     30561  1353509011\n",
      "2  20926447       3983      8428    789830  1353508616\n",
      "3  20926447       3983      8428    789384  1353508441\n",
      "4  20926447       3983      8428    789672  1353508189\n",
      "User-Tracks List DataFrame head:\n",
      "    user_id                                           track_id\n",
      "0    10879  [5011085, 5011086, 5011087, 5011088, 5011089, ...\n",
      "1  1002693  [3267123, 3267124, 3826841, 850917, 850918, 44...\n",
      "2  1022229  [4070595, 1989184, 102683, 517342, 5208, 40705...\n",
      "3  1026493  [5245580, 28025, 952133, 5245581, 5245582, 524...\n",
      "4  1048016  [637019, 11967, 240455, 1008317, 411879, 10083...\n",
      "User-Distinct Track Count DataFrame head:\n",
      "    user_id  distinct_track_count\n",
      "0    10879                  1454\n",
      "1  1002693                  8220\n",
      "2  1022229                  5509\n",
      "3  1026493                 17157\n",
      "4  1048016                  6787\n",
      "Users DataFrame head:\n",
      "    user_id country  age gender  playcount   timestamp\n",
      "0      384      UK   35      m      42139  1035849600\n",
      "1     1206     NaN   -1      n      33103  1035849600\n",
      "2     2622     NaN   -1    NaN       2030  1037404800\n",
      "3     2732     NaN   -1      n        147  1037577600\n",
      "4     3653      UK   31      m      18504  1041033600\n",
      "Users with Track IDs and Distinct Counts DataFrame head:\n",
      "    user_id country  age gender  playcount   timestamp track_id  \\\n",
      "0      384      UK   35      m      42139  1035849600      NaN   \n",
      "1     1206     NaN   -1      n      33103  1035849600      NaN   \n",
      "2     2622     NaN   -1    NaN       2030  1037404800      NaN   \n",
      "3     2732     NaN   -1      n        147  1037577600      NaN   \n",
      "4     3653      UK   31      m      18504  1041033600      NaN   \n",
      "\n",
      "   distinct_track_count  \n",
      "0                   NaN  \n",
      "1                   NaN  \n",
      "2                   NaN  \n",
      "3                   NaN  \n",
      "4                   NaN  \n",
      "Final DataFrame head:\n",
      "    user_id  playcount gender  distinct_track_count track_id\n",
      "0      384      42139      m                   NaN      NaN\n",
      "1     1206      33103      n                   NaN      NaN\n",
      "2     2622       2030    NaN                   NaN      NaN\n",
      "3     2732        147      n                   NaN      NaN\n",
      "4     3653      18504      m                   NaN      NaN\n"
     ]
    }
   ],
   "source": [
    "events_subset_path = \"/home/etaylor/code_projects/track2vec/data/events_part_107.csv\"\n",
    "\n",
    "final_dataset = create_training_dataset(events_subset_path, users_filepath, output_filepath, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records 2323\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>playcount</th>\n",
       "      <th>gender</th>\n",
       "      <th>distinct_track_count</th>\n",
       "      <th>track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10879</td>\n",
       "      <td>147520</td>\n",
       "      <td>n</td>\n",
       "      <td>1454.0</td>\n",
       "      <td>[5011085, 5011086, 5011087, 5011088, 5011089, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1002693</td>\n",
       "      <td>69223</td>\n",
       "      <td>n</td>\n",
       "      <td>8220.0</td>\n",
       "      <td>[3267123, 3267124, 3826841, 850917, 850918, 44...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1022229</td>\n",
       "      <td>91248</td>\n",
       "      <td>m</td>\n",
       "      <td>5509.0</td>\n",
       "      <td>[4070595, 1989184, 102683, 517342, 5208, 40705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1026493</td>\n",
       "      <td>13102</td>\n",
       "      <td>m</td>\n",
       "      <td>17157.0</td>\n",
       "      <td>[5245580, 28025, 952133, 5245581, 5245582, 524...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1048016</td>\n",
       "      <td>22963</td>\n",
       "      <td>m</td>\n",
       "      <td>6787.0</td>\n",
       "      <td>[637019, 11967, 240455, 1008317, 411879, 10083...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  playcount gender  distinct_track_count  \\\n",
       "13     10879     147520      n                1454.0   \n",
       "33   1002693      69223      n                8220.0   \n",
       "73   1022229      91248      m                5509.0   \n",
       "85   1026493      13102      m               17157.0   \n",
       "149  1048016      22963      m                6787.0   \n",
       "\n",
       "                                              track_id  \n",
       "13   [5011085, 5011086, 5011087, 5011088, 5011089, ...  \n",
       "33   [3267123, 3267124, 3826841, 850917, 850918, 44...  \n",
       "73   [4070595, 1989184, 102683, 517342, 5208, 40705...  \n",
       "85   [5245580, 28025, 952133, 5245581, 5245582, 524...  \n",
       "149  [637019, 11967, 240455, 1008317, 411879, 10083...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(f\"Number of records {len(final_dataset)}\")\n",
    "final_dataset.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
